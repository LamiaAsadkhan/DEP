{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_h-xHIxoYqm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv('CloudWatch_Traffic_Web_Attack.csv')\n",
        "\n",
        "# Data Cleaning\n",
        "\n",
        "# Handle missing values\n",
        "data.fillna(method='ffill', inplace=True)\n",
        "\n",
        "# Remove duplicates\n",
        "data.drop_duplicates(inplace=True)\n",
        "\n",
        "# Filter irrelevant columns\n",
        "data.drop(columns=['rule_names', 'observation_name', 'source.meta', 'source.name', 'detection_types'], inplace=True)\n",
        "\n",
        "# Timestamp Conversion\n",
        "data['creation_time'] = pd.to_datetime(data['creation_time'])\n",
        "data['end_time'] = pd.to_datetime(data['end_time'])\n",
        "data['time'] = pd.to_datetime(data['time'])\n",
        "\n",
        "# Extract additional time-based features\n",
        "data['creation_day_of_week'] = data['creation_time'].dt.dayofweek\n",
        "data['creation_hour'] = data['creation_time'].dt.hour\n",
        "data['end_day_of_week'] = data['end_time'].dt.dayofweek\n",
        "data['end_hour'] = data['end_time'].dt.hour\n",
        "\n",
        "# Feature Extraction\n",
        "features = ['bytes_in', 'bytes_out', 'protocol', 'dst_port', 'response.code', 'creation_day_of_week', 'creation_hour', 'end_day_of_week', 'end_hour']\n",
        "X = data[features]\n",
        "\n",
        "# Data Normalization\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Print the first few rows of the preprocessed data\n",
        "print(pd.DataFrame(X_scaled, columns=features).head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection and preprocessing"
      ],
      "metadata": {
        "id": "kfTkhHb-pQST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data = pd.read_csv('CloudWatch_Traffic_Web_Attack.csv')\n",
        "\n",
        "# Handle missing values\n",
        "data.fillna(method='ffill', inplace=True)\n",
        "\n",
        "# Remove duplicates\n",
        "data.drop_duplicates(inplace=True)\n",
        "\n",
        "# Filter irrelevant columns\n",
        "data.drop(columns=['rule_names', 'observation_name', 'source.meta', 'source.name', 'detection_types'], inplace=True)\n"
      ],
      "metadata": {
        "id": "SatIBYwhpaqg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Timestamp Conversion\n",
        "data['creation_time'] = pd.to_datetime(data['creation_time'])\n",
        "data['end_time'] = pd.to_datetime(data['end_time'])\n",
        "data['time'] = pd.to_datetime(data['time'])\n",
        "\n",
        "# Extract additional time-based features\n",
        "data['creation_day_of_week'] = data['creation_time'].dt.dayofweek\n",
        "data['creation_hour'] = data['creation_time'].dt.hour\n",
        "data['end_day_of_week'] = data['end_time'].dt.dayofweek\n",
        "data['end_hour'] = data['end_time'].dt.hour"
      ],
      "metadata": {
        "id": "ehb9NbCvpqoe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Extraction\n",
        "features = ['bytes_in', 'bytes_out', 'dst_port', 'response.code', 'creation_day_of_week', 'creation_hour', 'end_day_of_week', 'end_hour']  # Removed 'protocol'\n",
        "X = data[features]\n",
        "\n",
        "# Data Normalization\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(pd.DataFrame(X_scaled, columns=features).head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mr7_DAofpspn",
        "outputId": "81ae1409-c2c3-4be3-9421-c0089ae59681"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   bytes_in  bytes_out  dst_port  response.code  creation_day_of_week  \\\n",
            "0 -0.288219  -0.281223       0.0            0.0             -1.965215   \n",
            "1 -0.282108  -0.260804       0.0            0.0             -1.965215   \n",
            "2 -0.282689  -0.279344       0.0            0.0             -1.965215   \n",
            "3 -0.282197  -0.276161       0.0            0.0             -1.965215   \n",
            "4 -0.287996  -0.277678       0.0            0.0             -1.965215   \n",
            "\n",
            "   creation_hour  end_day_of_week  end_hour  \n",
            "0       1.766389         -2.18062  1.918236  \n",
            "1       1.766389         -2.18062  1.918236  \n",
            "2       1.766389         -2.18062  1.918236  \n",
            "3       1.766389         -2.18062  1.918236  \n",
            "4       1.766389         -2.18062  1.918236  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Derived Features\n",
        "data['packet_size'] = data['bytes_in'] + data['bytes_out']\n",
        "\n",
        "# Time intervals between packets (assuming data is sorted by creation_time)\n",
        "data.sort_values(by='creation_time', inplace=True)\n",
        "data['time_interval'] = data['creation_time'].diff().dt.total_seconds().fillna(0)\n",
        "\n",
        "# Aggregated statistics over fixed intervals (e.g., 1 minute)\n",
        "data.set_index('creation_time', inplace=True)\n",
        "data['bytes_in_mean'] = data['bytes_in'].rolling('1T').mean().fillna(0)\n",
        "data['bytes_out_mean'] = data['bytes_out'].rolling('1T').mean().fillna(0)\n",
        "data['bytes_in_max'] = data['bytes_in'].rolling('1T').max().fillna(0)\n",
        "data['bytes_out_max'] = data['bytes_out'].rolling('1T').max().fillna(0)\n",
        "data.reset_index(inplace=True)\n"
      ],
      "metadata": {
        "id": "nrp-k_DPqSX2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Behavioral Features\n",
        "\n",
        "# Number of connections per source IP (behavioral feature)\n",
        "data['connections_per_ip'] = data.groupby('src_ip')['src_ip'].transform('count')\n",
        "\n",
        "# Ratio of incoming to outgoing bytes (behavioral feature)\n",
        "data['bytes_ratio'] = data['bytes_in'] / (data['bytes_out'] + 1)  # Add 1 to avoid division by zero\n",
        "\n",
        "# Frequency of response codes (behavioral feature)\n",
        "data['response_code_freq'] = data.groupby('response.code')['response.code'].transform('count')\n",
        "\n",
        "# Feature Extraction\n",
        "features = ['bytes_in', 'bytes_out', 'dst_port', 'response.code',\n",
        "            'creation_day_of_week', 'creation_hour', 'end_day_of_week', 'end_hour',\n",
        "            'packet_size', 'time_interval', 'bytes_in_mean', 'bytes_out_mean',\n",
        "            'bytes_in_max', 'bytes_out_max', 'connections_per_ip', 'bytes_ratio',\n",
        "            'response_code_freq']\n",
        "X = data[features]\n",
        "\n",
        "# Data Normalization\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Print the first few rows of the preprocessed data\n",
        "print(pd.DataFrame(X_scaled, columns=features).head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LaZ8pBLsl8H",
        "outputId": "12accd75-022a-429a-ec41-01d21aba4559"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   bytes_in  bytes_out  dst_port  response.code  creation_day_of_week  \\\n",
            "0 -0.288219  -0.281223       0.0            0.0             -1.965215   \n",
            "1 -0.282108  -0.260804       0.0            0.0             -1.965215   \n",
            "2 -0.282689  -0.279344       0.0            0.0             -1.965215   \n",
            "3 -0.282197  -0.276161       0.0            0.0             -1.965215   \n",
            "4 -0.287996  -0.277678       0.0            0.0             -1.965215   \n",
            "\n",
            "   creation_hour  end_day_of_week  end_hour  packet_size  time_interval  \\\n",
            "0       1.766389         -2.18062  1.918236    -0.287850      -0.104119   \n",
            "1       1.766389         -2.18062  1.918236    -0.280910      -0.104119   \n",
            "2       1.766389         -2.18062  1.918236    -0.282531      -0.104119   \n",
            "3       1.766389         -2.18062  1.918236    -0.281883      -0.104119   \n",
            "4       1.766389         -2.18062  1.918236    -0.287435      -0.104119   \n",
            "\n",
            "   bytes_in_mean  bytes_out_mean  bytes_in_max  bytes_out_max  \\\n",
            "0      -0.694129       -0.679348     -0.848944      -0.841860   \n",
            "1      -0.687025       -0.655542     -0.845687      -0.830983   \n",
            "2      -0.685108       -0.662017     -0.845687      -0.830983   \n",
            "3      -0.683863       -0.663399     -0.845687      -0.830983   \n",
            "4      -0.685812       -0.664936     -0.845687      -0.830983   \n",
            "\n",
            "   connections_per_ip  bytes_ratio  response_code_freq  \n",
            "0           -0.445092    -0.452492                 0.0  \n",
            "1           -0.936349    -0.292629                 0.0  \n",
            "2           -0.567907    -0.240107                 0.0  \n",
            "3           -0.813535    -0.237232                 0.0  \n",
            "4           -0.199464    -0.447639                 0.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Anomaly detection algorithms (e.g., Isolation Forest,\n",
        "Autoencoders)"
      ],
      "metadata": {
        "id": "eJJ6QkLItw1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras import backend as K\n",
        "# Isolation Forest\n",
        "iso_forest = IsolationForest(contamination=0.05)\n",
        "iso_forest.fit(X_scaled)\n",
        "anomalies_if = iso_forest.predict(X_scaled)\n",
        "data['anomaly_if'] = anomalies_if\n",
        "\n",
        "# Autoencoder\n",
        "def create_autoencoder(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(32, activation='relu', input_dim=input_dim))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(8, activation='relu'))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(input_dim, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "input_dim = X_scaled.shape[1]\n",
        "autoencoder = create_autoencoder(input_dim)\n",
        "\n",
        "# Train the autoencoder\n",
        "autoencoder.fit(X_scaled, X_scaled, epochs=50, batch_size=32, validation_split=0.1, verbose=1)\n",
        "\n",
        "# Reconstruction error\n",
        "reconstructions = autoencoder.predict(X_scaled)\n",
        "reconstruction_error = K.mean(K.square(reconstructions - X_scaled), axis=1)\n",
        "threshold = K.mean(reconstruction_error) + 2 * K.std(reconstruction_error)\n",
        "anomalies_ae = (reconstruction_error > threshold).numpy()\n",
        "data['anomaly_ae'] = anomalies_ae\n",
        "\n",
        "# Print the first few rows of anomalies\n",
        "print(data[['anomaly_if', 'anomaly_ae']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKos0Y5jsuWk",
        "outputId": "424be348-cd2d-41f1-d55c-c3cc9ba1e026"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "8/8 [==============================] - 2s 48ms/step - loss: 1.0477 - val_loss: 1.2199\n",
            "Epoch 2/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1.0315 - val_loss: 1.2109\n",
            "Epoch 3/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1.0100 - val_loss: 1.1972\n",
            "Epoch 4/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.9798 - val_loss: 1.1755\n",
            "Epoch 5/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.9390 - val_loss: 1.1476\n",
            "Epoch 6/50\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.8873 - val_loss: 1.1154\n",
            "Epoch 7/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.8306 - val_loss: 1.0785\n",
            "Epoch 8/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.7760 - val_loss: 1.0420\n",
            "Epoch 9/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.7330 - val_loss: 1.0085\n",
            "Epoch 10/50\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.7010 - val_loss: 0.9739\n",
            "Epoch 11/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.6763 - val_loss: 0.9437\n",
            "Epoch 12/50\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.6555 - val_loss: 0.9217\n",
            "Epoch 13/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.6401 - val_loss: 0.9048\n",
            "Epoch 14/50\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.6270 - val_loss: 0.8883\n",
            "Epoch 15/50\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.6160 - val_loss: 0.8756\n",
            "Epoch 16/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.6080 - val_loss: 0.8664\n",
            "Epoch 17/50\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.6019 - val_loss: 0.8608\n",
            "Epoch 18/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5971 - val_loss: 0.8563\n",
            "Epoch 19/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5924 - val_loss: 0.8525\n",
            "Epoch 20/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5885 - val_loss: 0.8480\n",
            "Epoch 21/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.5855 - val_loss: 0.8451\n",
            "Epoch 22/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.5829 - val_loss: 0.8436\n",
            "Epoch 23/50\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 0.5805 - val_loss: 0.8411\n",
            "Epoch 24/50\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 0.5781 - val_loss: 0.8374\n",
            "Epoch 25/50\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5756 - val_loss: 0.8356\n",
            "Epoch 26/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.5729 - val_loss: 0.8314\n",
            "Epoch 27/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.5694 - val_loss: 0.8267\n",
            "Epoch 28/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5655 - val_loss: 0.8217\n",
            "Epoch 29/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5614 - val_loss: 0.8144\n",
            "Epoch 30/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5572 - val_loss: 0.8049\n",
            "Epoch 31/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5526 - val_loss: 0.7959\n",
            "Epoch 32/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5485 - val_loss: 0.7832\n",
            "Epoch 33/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5442 - val_loss: 0.7713\n",
            "Epoch 34/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5396 - val_loss: 0.7570\n",
            "Epoch 35/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5352 - val_loss: 0.7446\n",
            "Epoch 36/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5306 - val_loss: 0.7326\n",
            "Epoch 37/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5265 - val_loss: 0.7199\n",
            "Epoch 38/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5221 - val_loss: 0.7117\n",
            "Epoch 39/50\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 0.5180 - val_loss: 0.7033\n",
            "Epoch 40/50\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5148 - val_loss: 0.6996\n",
            "Epoch 41/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5117 - val_loss: 0.6928\n",
            "Epoch 42/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5088 - val_loss: 0.6943\n",
            "Epoch 43/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5066 - val_loss: 0.6874\n",
            "Epoch 44/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5043 - val_loss: 0.6894\n",
            "Epoch 45/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5027 - val_loss: 0.6868\n",
            "Epoch 46/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.5009 - val_loss: 0.6864\n",
            "Epoch 47/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.4994 - val_loss: 0.6876\n",
            "Epoch 48/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.4982 - val_loss: 0.6836\n",
            "Epoch 49/50\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.4973 - val_loss: 0.6851\n",
            "Epoch 50/50\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.4963 - val_loss: 0.6844\n",
            "9/9 [==============================] - 0s 2ms/step\n",
            "   anomaly_if  anomaly_ae\n",
            "0           1       False\n",
            "1           1       False\n",
            "2           1       False\n",
            "3           1       False\n",
            "4           1       False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation and validation of detected anomalies"
      ],
      "metadata": {
        "id": "65ZhDSDSt8rs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "data['true_labels'] = 0\n",
        "data.loc[50:100, 'true_labels'] = 1\n",
        "\n",
        "# Evaluate using the created true labels\n",
        "y_true = data['true_labels']\n",
        "\n",
        "# Isolation Forest evaluation\n",
        "print(\"Isolation Forest Evaluation\")\n",
        "print(confusion_matrix(y_true, data['anomaly_if']))\n",
        "print(classification_report(y_true, data['anomaly_if']))\n",
        "\n",
        "# Autoencoder evaluation\n",
        "print(\"Autoencoder Evaluation\")\n",
        "print(confusion_matrix(y_true, data['anomaly_ae']))\n",
        "print(classification_report(y_true, data['anomaly_ae']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSFmNuqTuB_l",
        "outputId": "6ba90c65-c300-4359-ba16-f8c8626ac456"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Isolation Forest Evaluation\n",
            "[[  0   0   0]\n",
            " [ 15   0 216]\n",
            " [  0   0  51]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.00      0.00      0.00         0\n",
            "           0       0.00      0.00      0.00       231\n",
            "           1       0.19      1.00      0.32        51\n",
            "\n",
            "    accuracy                           0.18       282\n",
            "   macro avg       0.06      0.33      0.11       282\n",
            "weighted avg       0.03      0.18      0.06       282\n",
            "\n",
            "Autoencoder Evaluation\n",
            "[[226   5]\n",
            " [ 51   0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.98      0.89       231\n",
            "           1       0.00      0.00      0.00        51\n",
            "\n",
            "    accuracy                           0.80       282\n",
            "   macro avg       0.41      0.49      0.44       282\n",
            "weighted avg       0.67      0.80      0.73       282\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZuQxMicgtqv8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}